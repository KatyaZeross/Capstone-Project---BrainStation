{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2cfa1a-64f9-4bfb-be09-1275b0354c03",
   "metadata": {},
   "source": [
    "# Capstone Project: Star Trek IMDB Ratings Predictor - Katya Kogan - Part 2\n",
    "--- \n",
    "## Introduction\n",
    "\n",
    "Now we've successfully cleaned and prepared our data, we can begin our modelling process to determine if the characters (by their spoken parts) contribute to the `imdbRating`. \n",
    "\n",
    "Since we are focusing on feature importance, our models will have to be carefully chosen, as this is a regression problem - so we'll use Regression models such as: Linear Regression, Decision Tree Regressor, Random Forest Regressor, SVM (Regressor), OrdinalRidge Regression (MORD), Least Absolute Deviation (MORD), and Gradient Boosting Regressor. \n",
    "\n",
    "We'll be performing baseline tests across these models, and then find the one performs that best and fine-tune parameters, which afterwords - we'll be testing the precision and recall of them. \n",
    "\n",
    "We'll begin by importing all the relevant libraries needed for our project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b100848-e90f-4a69-98e0-511da9737c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "#ML libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#Importing Mord library\n",
    "from mord import LogisticIT, LogisticAT, LAD, OrdinalRidge\n",
    "\n",
    "# libraries for splitting and preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import linear_model as lm, metrics, tree, ensemble, model_selection as ms, feature_selection, svm\n",
    "\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "# Ignore futurewarnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5298f74-2654-4815-baaf-ae97da035622",
   "metadata": {},
   "source": [
    "With our libraries imported, we can begin by reading in our dataset, do some minor EDA and prep before starting to fit our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53dabd1f-72ba-43e9-9163-17bce17fc1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_df = pd.read_csv(r\"C:\\Users\\Katya\\Documents\\GitHub\\Capstone-Project---BrainStation\\PCT_Graph.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3818ce55-9e05-4620-b2e0-0beda5b0be09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PICARD_PCT</th>\n",
       "      <th>RIKER_PCT</th>\n",
       "      <th>DATA_PCT</th>\n",
       "      <th>WORF_PCT</th>\n",
       "      <th>TROI_PCT</th>\n",
       "      <th>BEVERLY_PCT</th>\n",
       "      <th>GEORDI_PCT</th>\n",
       "      <th>Q_PCT</th>\n",
       "      <th>LORE_PCT</th>\n",
       "      <th>WESLEY_PCT</th>\n",
       "      <th>GUINAN_PCT</th>\n",
       "      <th>TASHA_PCT</th>\n",
       "      <th>PULASKI_PCT</th>\n",
       "      <th>RATING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>33.46</td>\n",
       "      <td>31.58</td>\n",
       "      <td>10.44</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.26</td>\n",
       "      <td>5.21</td>\n",
       "      <td>6.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11.47</td>\n",
       "      <td>3.01</td>\n",
       "      <td>26.08</td>\n",
       "      <td>25.44</td>\n",
       "      <td>14.94</td>\n",
       "      <td>4.23</td>\n",
       "      <td>14.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.996689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>27.32</td>\n",
       "      <td>42.75</td>\n",
       "      <td>7.88</td>\n",
       "      <td>8.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.25</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>39.13</td>\n",
       "      <td>24.34</td>\n",
       "      <td>11.60</td>\n",
       "      <td>1.94</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.92</td>\n",
       "      <td>16.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>41.64</td>\n",
       "      <td>9.67</td>\n",
       "      <td>18.77</td>\n",
       "      <td>5.32</td>\n",
       "      <td>2.31</td>\n",
       "      <td>5.44</td>\n",
       "      <td>16.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  PICARD_PCT  RIKER_PCT  DATA_PCT  WORF_PCT  TROI_PCT  \\\n",
       "0           0       33.46      31.58     10.44      1.69      0.00   \n",
       "1           1       11.47       3.01     26.08     25.44     14.94   \n",
       "2           2       27.32      42.75      7.88      8.60      0.00   \n",
       "3           3       39.13      24.34     11.60      1.94      3.09   \n",
       "4           4       41.64       9.67     18.77      5.32      2.31   \n",
       "\n",
       "   BEVERLY_PCT  GEORDI_PCT  Q_PCT  LORE_PCT  WESLEY_PCT  GUINAN_PCT  \\\n",
       "0         4.26        5.21   6.05       0.0        4.52         0.0   \n",
       "1         4.23       14.82   0.00       0.0        0.00         0.0   \n",
       "2         0.00        0.00   0.00       0.0       10.20         0.0   \n",
       "3         0.92       16.36   0.00       0.0        2.63         0.0   \n",
       "4         5.44       16.85   0.00       0.0        0.00         0.0   \n",
       "\n",
       "   TASHA_PCT  PULASKI_PCT    RATING  \n",
       "0       2.79         0.00  8.000000  \n",
       "1       0.00         0.00  7.996689  \n",
       "2       0.00         3.25  8.000000  \n",
       "3       0.00         0.00  7.000000  \n",
       "4       0.00         0.00  7.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ST_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f8cc2d-7599-4aad-9bdc-147ee4d29bee",
   "metadata": {
    "id": "l_EKix33G3_H"
   },
   "source": [
    "Now we've successfully imported our dataframe, we can see that there is still some minor cleaning to do. We'll have to drop the `Unnamed: 0` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce0fec76-d4d6-46a2-9629-72cbd3d60584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the Unnamed: 0 column\n",
    "ST_df.drop(labels='Unnamed: 0', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7722a5-ac80-4996-ab8f-443bde56eb2e",
   "metadata": {
    "id": "TJ1kdJeKHjoR"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PICARD_PCT</th>\n",
       "      <th>RIKER_PCT</th>\n",
       "      <th>DATA_PCT</th>\n",
       "      <th>WORF_PCT</th>\n",
       "      <th>TROI_PCT</th>\n",
       "      <th>BEVERLY_PCT</th>\n",
       "      <th>GEORDI_PCT</th>\n",
       "      <th>Q_PCT</th>\n",
       "      <th>LORE_PCT</th>\n",
       "      <th>WESLEY_PCT</th>\n",
       "      <th>GUINAN_PCT</th>\n",
       "      <th>TASHA_PCT</th>\n",
       "      <th>PULASKI_PCT</th>\n",
       "      <th>RATING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.46</td>\n",
       "      <td>31.58</td>\n",
       "      <td>10.44</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.26</td>\n",
       "      <td>5.21</td>\n",
       "      <td>6.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.47</td>\n",
       "      <td>3.01</td>\n",
       "      <td>26.08</td>\n",
       "      <td>25.44</td>\n",
       "      <td>14.94</td>\n",
       "      <td>4.23</td>\n",
       "      <td>14.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.996689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.32</td>\n",
       "      <td>42.75</td>\n",
       "      <td>7.88</td>\n",
       "      <td>8.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.25</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39.13</td>\n",
       "      <td>24.34</td>\n",
       "      <td>11.60</td>\n",
       "      <td>1.94</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.92</td>\n",
       "      <td>16.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.64</td>\n",
       "      <td>9.67</td>\n",
       "      <td>18.77</td>\n",
       "      <td>5.32</td>\n",
       "      <td>2.31</td>\n",
       "      <td>5.44</td>\n",
       "      <td>16.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PICARD_PCT  RIKER_PCT  DATA_PCT  WORF_PCT  TROI_PCT  BEVERLY_PCT  \\\n",
       "0       33.46      31.58     10.44      1.69      0.00         4.26   \n",
       "1       11.47       3.01     26.08     25.44     14.94         4.23   \n",
       "2       27.32      42.75      7.88      8.60      0.00         0.00   \n",
       "3       39.13      24.34     11.60      1.94      3.09         0.92   \n",
       "4       41.64       9.67     18.77      5.32      2.31         5.44   \n",
       "\n",
       "   GEORDI_PCT  Q_PCT  LORE_PCT  WESLEY_PCT  GUINAN_PCT  TASHA_PCT  \\\n",
       "0        5.21   6.05       0.0        4.52         0.0       2.79   \n",
       "1       14.82   0.00       0.0        0.00         0.0       0.00   \n",
       "2        0.00   0.00       0.0       10.20         0.0       0.00   \n",
       "3       16.36   0.00       0.0        2.63         0.0       0.00   \n",
       "4       16.85   0.00       0.0        0.00         0.0       0.00   \n",
       "\n",
       "   PULASKI_PCT    RATING  \n",
       "0         0.00  8.000000  \n",
       "1         0.00  7.996689  \n",
       "2         3.25  8.000000  \n",
       "3         0.00  7.000000  \n",
       "4         0.00  7.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "ST_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bab28f-945e-464c-9243-19d6d1e4d05a",
   "metadata": {},
   "source": [
    "Great, we've dropped the unnecessary column. Let's take a quick look at the rating distribution, once more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8494be20-51e8-4a84-8767-8c3f2b81f3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# looking at the distribution of the IMDB Ratings\n",
    "values, counts = np.unique(ST_df['RATING'], return_counts=True)\n",
    "normalized_reviews = counts/counts.sum()\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(values, normalized_reviews * 100, edgecolor='black', color='#002193')\n",
    "plt.xlabel('imdbRating')\n",
    "plt.ylabel('% of Episodes')\n",
    "sns.despine()\n",
    "plt.title(\"IMDB Ratings Distribution\")\n",
    "plt.savefig(\"IMDB Ratings Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bf5dbc-9f26-485c-8ca8-d36d864b2f80",
   "metadata": {},
   "source": [
    "Now we'll take a renewed look at the `RATING` column, and make sure all the values are rounded up correctly (and remains a float). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "039fe291-180b-48ff-b5de-925383987726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.        , 7.99668874, 7.        , 6.        , 9.        ,\n",
       "       5.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check\n",
    "ST_df['RATING'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "710d5945-7b5b-4d43-ae62-5567aecd24d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rounding up the rating values\n",
    "ST_df['RATING'] = round(ST_df['RATING'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a0c459b-ab19-490d-87d2-8bd932fd5bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8., 7., 6., 9., 5.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check \n",
    "ST_df['RATING'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84e042f9-567a-49cd-a611-4f2dd33d99c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the type\n",
    "ST_df['RATING'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9640be-9b46-4780-bf0f-c80888b7d30a",
   "metadata": {},
   "source": [
    "Moving on, we can be confident going ahead with splitting the data set into train and test sets.\n",
    "\n",
    "After the splitting is completed, the training set(s) will be fit and transformed. Sidenote: the testing data shouldn't be mixed with the fit and transformation of the data. The fitting and transformation process will be applied separately to the `RATING` column.\n",
    "\n",
    "Essentially:\n",
    "\n",
    "1. Split `ST_df` into train and test sets\n",
    "2. With the fitted `RATING`, transform the `RATING` column for both X_train and X_test\n",
    "\n",
    "We'll begin by defining our X and y values:\n",
    "\n",
    "X : all columns except for `RATING` (independent)\n",
    "y : contains only `RATING` (dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1328745f-608a-4f93-9275-b800dd73e36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate df into features and target variable\n",
    "y = ST_df['RATING']\n",
    "# drop the target variable from the X dataframe (remaining features)\n",
    "X = ST_df.drop('RATING',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b77c9ab-005c-4d64-8378-09c60f232b3b",
   "metadata": {},
   "source": [
    "Now that we've separated the independent and dependent variables from each other, now we'll performing a train test split using a test size of 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49a21267-2e4d-4b67-9457-1c488018d403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test size of 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74680c05-650a-43ad-9514-ab97c75bf3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Proportions\n",
      "8.0    0.381503\n",
      "7.0    0.335260\n",
      "6.0    0.196532\n",
      "9.0    0.075145\n",
      "5.0    0.011561\n",
      "Name: RATING, dtype: float64 \n",
      "\n",
      "Training Data Proportions\n",
      "8.0    0.380165\n",
      "7.0    0.338843\n",
      "6.0    0.198347\n",
      "9.0    0.074380\n",
      "5.0    0.008264\n",
      "Name: RATING, dtype: float64 \n",
      "\n",
      "Testing Data Proportions\n",
      "8.0    0.384615\n",
      "7.0    0.326923\n",
      "6.0    0.192308\n",
      "9.0    0.076923\n",
      "5.0    0.019231\n",
      "Name: RATING, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# class proportion in the original\n",
    "print(\"Original Data Proportions\")\n",
    "print(y.value_counts()/len(y), '\\n')\n",
    "\n",
    "# class proportion in the training data\n",
    "print(\"Training Data Proportions\")\n",
    "print(y_train.value_counts()/len(y_train), '\\n')\n",
    "\n",
    "# class proportion of the testing data\n",
    "print(\"Testing Data Proportions\")\n",
    "print(y_test.value_counts()/len(y_test), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c598ab8b-584f-4c56-badb-d6d50a1c7795",
   "metadata": {},
   "source": [
    "It's good practice to standardise the predicative variables, especially for liner regressions. We'll fit and transform on the training set, and transform on the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6b84eb8-fed3-4a7f-bd63-5200bfb94917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training sets: 121 52\n",
      "Length of testing sets: 121 52\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)\n",
    "print(\"Length of training sets:\",len(X_train), len(X_test))\n",
    "print(\"Length of testing sets:\",len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01652ef-bfd1-4241-b4e5-b0222f3fcf14",
   "metadata": {},
   "source": [
    "### Modelling Workflow \n",
    "\n",
    "Now that we've successfully split the data set into their respective train and test datasets, we can commence comfortably with modelling. \n",
    "\n",
    "Our workflow will be:\n",
    "- Linear Regression\n",
    "- Decision Tree Regression\n",
    "- Random Forest Regression\n",
    "- SVM Regressor\n",
    "- OrdinalRidge Regression\n",
    "- LAD \n",
    "\n",
    "Let's start with the baseline models, determine which ones perform the best and then start tuning the hyperparameters. Since we're testing a myriad of models, I decided to create a function where it'll take in the train-test data, and the model type in order to return the default model scores. It'll return the train-test scores (using RMSE as an evaluation metric), cross validation scores, and the mean cross validation score. \n",
    "\n",
    "In addition to this, another function will also be created for regularising/hyperparameter optimization once we've selected our model to dive deeper into. \n",
    "\n",
    "Let's create our simple model metrics function, as well as our empty lists to store the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b0e5ad2-06f5-4694-babd-1ab5f39cd620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating our lists to store the results of our default models \n",
    "training_score = []\n",
    "testing_RMSE = []\n",
    "mean_cross_val_score = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2725ac37-c1c7-4202-96ac-f70b627a024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return simple model metrics\n",
    "def simple_model_metrics(X_train, y_train, X_test, y_test, model, parametric=True):\n",
    "    '''Takes in train test splits and the model type, and returns the training and test score (RMSE),\n",
    "    cross-validation scores, and the mean cross-validation score. Returns the feature importances if the \n",
    "    argument \"parametric\" is True or False''' \n",
    "    \n",
    "    # fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # predict the models\n",
    "    train_pred = np.around(model.predict(X_train),1)\n",
    "    test_pred = np.around(model.predict(X_test),1)\n",
    "    \n",
    "    # print results using RMSE for train and test scores\n",
    "    print('Training RMSE', '{0:0.2f}'.format(np.sqrt(metrics.mean_squared_error(y_train, train_pred))))\n",
    "    print('Testing RMSE:', '{0:0.2f}'.format(np.sqrt(metrics.mean_squared_error(y_test, test_pred))))\n",
    "    \n",
    "    # append train and testing scores to list\n",
    "    training_score.append('{0:0.2f}'.format(np.sqrt(metrics.mean_squared_error(y_train, train_pred))))\n",
    "    testing_RMSE.append('{0:0.2f}'.format(np.sqrt(metrics.mean_squared_error(y_test, test_pred))))\n",
    "    \n",
    "    # acquire the cross validation score (neg mean squared error), 5 folds\n",
    "    cv_scores = -ms.cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    # print results of the cross val RMSEs, and the mean cross val RMSE\n",
    "    print('Cross-validated RMSEs:', np.sqrt(cv_scores))\n",
    "    print('Mean cross-validated RMSE:', '{0:0.2f}'.format(np.sqrt(np.mean(cv_scores))))\n",
    "    \n",
    "    # appending mean cross val RMSE to list\n",
    "    mean_cross_val_score.append('{0:0.2f}'.format(np.sqrt(np.mean(cv_scores))))\n",
    "    \n",
    "    # conditions if parametric is true or false\n",
    "    if parametric == True:\n",
    "        # prints a dataframe containing the X train cols, model coefficients and absolute coefficient\n",
    "        print(pd.DataFrame(list(zip(X_train.columns, model.coef_, abs(model.coef_))), \n",
    "                 columns=['Feature', 'Coef', 'Abs Coef']).sort_values('Abs Coef', ascending=False).head(10))\n",
    "    else:\n",
    "        # prints a dataframe containing the X train cols and model feature importances\n",
    "        print(pd.DataFrame(list(zip(X_train.columns, model.feature_importances_)), \n",
    "                 columns=['Feature', 'Importance']).sort_values('Importance', ascending=False).head(10))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a23406-2b60-4b2e-b2af-73bf2f5826ef",
   "metadata": {},
   "source": [
    "#### Baseline RMSE\n",
    "With our function created, now we can use it quite simply and with ease. However, we don't know what baseline to compare the RMSE scores with. This score explains what you would get if the mean value for the `y` was predicted. Essentially, if the model does better than this score - it's a good start! \n",
    "\n",
    "Let's check what the baseline is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a45918f8-ae6f-442b-b658-dc1282337abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline score RMSE:  0.93\n"
     ]
    }
   ],
   "source": [
    "y_pred_mean = [y_train.mean()] * len(y_test)\n",
    "\n",
    "print(\"Baseline score RMSE: \",'{0:0.2f}'.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred_mean))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196585c3-38e9-4df8-8b3c-42b11ba586e8",
   "metadata": {},
   "source": [
    "The baseline score is an RMSE of 0.93, meaning that the predicted score is within 0.93 of a mark out of 1. This is a baseline to compare the performance of my models. The current RMSE suggests that there is variance in the data that would lead to this outcome. \n",
    "\n",
    "Since this is a regression problem, we're referring to the Root Mean Squared Error as our score dimension. We'll be focusing on the models that have the best performing cross-validation scores, while taking into consideration of the training and test scores (checking for underfitting or overfitting). \n",
    "\n",
    "To intrepret the RMSE, the rule of thumb is that if the predicted and true values differ by a large margin - the RMSE will be large. If the RMSE is close to 0, it's nearing a perfect fit of the data. \n",
    "\n",
    "To start us off, we'll begin with a few simple models and compare their scores (default). Once we determine which one performs the best, we'll dive into it a bit deeper with a pipeline (using a function). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc4efca-1346-40a6-a45d-1bc5bcc869b7",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38f41c23-0db6-4561-8c3f-e4567725fc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE 0.81\n",
      "Testing RMSE: 0.87\n",
      "Cross-validated RMSEs: [1.00363983 0.94040328 0.99844307 0.90575722 0.97937206]\n",
      "Mean cross-validated RMSE: 0.97\n",
      "        Feature        Coef    Abs Coef\n",
      "0    PICARD_PCT -213.374965  213.374965\n",
      "2      DATA_PCT -160.190173  160.190173\n",
      "6    GEORDI_PCT -139.984872  139.984872\n",
      "5   BEVERLY_PCT -134.309799  134.309799\n",
      "1     RIKER_PCT -125.320469  125.320469\n",
      "4      TROI_PCT -123.563818  123.563818\n",
      "3      WORF_PCT -118.513233  118.513233\n",
      "7         Q_PCT -104.318121  104.318121\n",
      "9    WESLEY_PCT  -67.013172   67.013172\n",
      "11    TASHA_PCT  -42.593696   42.593696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_default = simple_model_metrics(X_train, y_train, X_test, y_test, LinearRegression())\n",
    "linear_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dc9983-b4b5-4b6c-86f9-4d5fd732226f",
   "metadata": {},
   "source": [
    "But the training and test RMSEs are a bit high, and the cross validation score is higher than our baseline. This suggests that the model is underfit, and regularisation could help with improving these results, but let's explore the other options first before we attempt to tune the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e1b1f-7ab9-4ade-a9a8-891a2933f00b",
   "metadata": {},
   "source": [
    "#### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5675ab5-cc62-44f5-80a8-fc1ccd630b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE 0.00\n",
      "Testing RMSE: 1.14\n",
      "Cross-validated RMSEs: [1.14891253 1.30703226 1.42886902 1.32287566 1.2416387 ]\n",
      "Mean cross-validated RMSE: 1.29\n",
      "        Feature  Importance\n",
      "3      WORF_PCT    0.209085\n",
      "4      TROI_PCT    0.196536\n",
      "0    PICARD_PCT    0.094095\n",
      "5   BEVERLY_PCT    0.091613\n",
      "11    TASHA_PCT    0.087755\n",
      "2      DATA_PCT    0.062841\n",
      "1     RIKER_PCT    0.062197\n",
      "9    WESLEY_PCT    0.061832\n",
      "10   GUINAN_PCT    0.053058\n",
      "6    GEORDI_PCT    0.040012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_default = simple_model_metrics(X_train, y_train, X_test, y_test, DecisionTreeRegressor(), False)\n",
    "tree_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f501f4-6dc9-4f65-85b7-9b9a24b04948",
   "metadata": {},
   "source": [
    "The training score is 0, and the testing score is exceedingly high. This suggests that this model is extremely overfit. We won't be considering this as an option to persue. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1957e71d-435b-4dcc-a0c5-8eb7efdada48",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "555e9138-5801-4d41-a32d-3a9453427f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE 0.35\n",
      "Testing RMSE: 0.87\n",
      "Cross-validated RMSEs: [1.01899951 0.84941647 0.94805063 0.94192843 0.88329638]\n",
      "Mean cross-validated RMSE: 0.93\n",
      "        Feature  Importance\n",
      "3      WORF_PCT    0.205968\n",
      "4      TROI_PCT    0.145247\n",
      "0    PICARD_PCT    0.111536\n",
      "1     RIKER_PCT    0.098299\n",
      "6    GEORDI_PCT    0.082832\n",
      "5   BEVERLY_PCT    0.079360\n",
      "2      DATA_PCT    0.070417\n",
      "9    WESLEY_PCT    0.062094\n",
      "11    TASHA_PCT    0.061400\n",
      "10   GUINAN_PCT    0.047818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_default = simple_model_metrics(X_train, y_train, X_test, y_test, RandomForestRegressor(), False)\n",
    "forest_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6a5b53-97b3-4ff6-a7bc-fe0045f3aed3",
   "metadata": {},
   "source": [
    "The training and test scores are both lower than the baseline is, including the cross validation score (by 0.01). This is already showing signs of a good fit, and this model is currently our best option. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab3ba8a-0994-4679-854b-b9273e0d4a4e",
   "metadata": {},
   "source": [
    "#### Support Vector Machines (Regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d795095-85f2-4dde-9e05-98865e1dbaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE: 0.62\n",
      "Testing RMSE: 0.94\n",
      "Cross-validated RMSEs: [0.79023991 0.77991106 0.67043417 0.72753278 0.81789065]\n",
      "Mean cross-validated RMSE: 0.76\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "svr_default = svm.SVR()\n",
    "\n",
    "#fit the model\n",
    "svr_default.fit(X_train, y_train)\n",
    "\n",
    "# print results from training and testing RMSE\n",
    "print('Training RMSE:', '{0:0.2f}'.format(np.sqrt(metrics.mean_squared_error(y_train, svr_default.predict(X_train)))))\n",
    "print('Testing RMSE:', '{0:0.2f}'.format(np.sqrt(metrics.mean_squared_error(y_test, svr_default.predict(X_test)))))\n",
    "\n",
    "# append train and testing scores to lists\n",
    "training_score.append('{0:0.2f}'.format(np.sqrt(metrics.mean_squared_error(y_train, svr_default.predict(X_train)))))\n",
    "testing_RMSE.append('{0:0.2f}'.format(np.sqrt(metrics.mean_squared_error(y_test, svr_default.predict(X_test)))))\n",
    "\n",
    "# acquire the cross validation scores\n",
    "cv_scores = -ms.cross_val_score(svr_default, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# printing the results of the cross val scores and the mean cross val score\n",
    "print('Cross-validated RMSEs:', cv_scores)\n",
    "print('Mean cross-validated RMSE:', '{0:0.2f}'.format(np.mean(cv_scores)))\n",
    "\n",
    "# appending mean cross val RMSE to list\n",
    "mean_cross_val_score.append('{0:0.2f}'.format(np.sqrt(np.mean(cv_scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1406270a-4d5c-4315-b2d8-966bbfa2284d",
   "metadata": {},
   "source": [
    "The training score is lower than our baseline, with the testing score matching the baseline. However, the cross validation score is what's stealing the show. With a RMSE of 0.76, this suggests to me that this model is another worthy contention worth persuing further. \n",
    "\n",
    "---\n",
    "\n",
    "There is a package that is dedicated for Ordinal Regression (goal is to predict a variable that is discrete and ordered). Referring to this [documentation](https://pythonhosted.org/mord/#regression-based), we'll test the defaults of the MORD package and see how it compares against the other models from sklearn. \n",
    "\n",
    "#### OrdinalRidge Regression - MORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1dcd083-4d4c-4d74-a7d9-e55536fb0828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE 0.85\n",
      "Testing RMSE: 0.93\n",
      "Cross-validated RMSEs: [1.07703296 0.95742711 1.09924216 0.88975652 1.08012345]\n",
      "Mean cross-validated RMSE: 1.02\n",
      "        Feature      Coef  Abs Coef\n",
      "10   GUINAN_PCT  0.197785  0.197785\n",
      "9    WESLEY_PCT -0.185553  0.185553\n",
      "4      TROI_PCT -0.144367  0.144367\n",
      "11    TASHA_PCT -0.131599  0.131599\n",
      "12  PULASKI_PCT -0.102575  0.102575\n",
      "8      LORE_PCT  0.089497  0.089497\n",
      "5   BEVERLY_PCT -0.086334  0.086334\n",
      "1     RIKER_PCT  0.082768  0.082768\n",
      "3      WORF_PCT  0.062255  0.062255\n",
      "0    PICARD_PCT  0.058793  0.058793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrdinalRidge()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mord_ordinal_default = simple_model_metrics(X_train, y_train, X_test, y_test, OrdinalRidge())\n",
    "mord_ordinal_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb33dc-8abb-48bf-98cf-75ed5d032c56",
   "metadata": {},
   "source": [
    "Even though the training and testing RMSE are lower than our baseline, the cross validation is way too high. We won't be considering this as an option. \n",
    "\n",
    "---\n",
    "#### LAD (Least Absolute Deviation) - MORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33fcb227-87d8-450b-9cba-4a8777621755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE 0.88\n",
      "Testing RMSE: 0.94\n",
      "Cross-validated RMSEs: [1.07703296 1.         0.91287093 0.93541435 1.17260394]\n",
      "Mean cross-validated RMSE: 1.02\n",
      "        Feature      Coef  Abs Coef\n",
      "4      TROI_PCT -0.218031  0.218031\n",
      "11    TASHA_PCT -0.188546  0.188546\n",
      "10   GUINAN_PCT  0.183016  0.183016\n",
      "12  PULASKI_PCT -0.177696  0.177696\n",
      "3      WORF_PCT  0.175703  0.175703\n",
      "6    GEORDI_PCT  0.137086  0.137086\n",
      "5   BEVERLY_PCT -0.130370  0.130370\n",
      "9    WESLEY_PCT -0.119352  0.119352\n",
      "2      DATA_PCT  0.082285  0.082285\n",
      "8      LORE_PCT  0.074150  0.074150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LAD()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mord_lad_default = simple_model_metrics(X_train, y_train, X_test, y_test, LAD())\n",
    "mord_lad_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae44e7b-75c6-45cb-b2f5-e4c519e7478a",
   "metadata": {},
   "source": [
    "The same applies to the LAD model as well, the training and test scores are also a little lower or similar to our baseline, but the cross validation is also too high. This is also another option that we won't consider as well. \n",
    "\n",
    "#### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a4ac8f8-ab63-4169-9881-325c9bc55999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "017ce587-7693-4a1c-aebc-11143d84a240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE 0.30\n",
      "Testing RMSE: 0.91\n",
      "Cross-validated RMSEs: [1.10544474 0.85880973 1.20853926 1.11539648 0.93142042]\n",
      "Mean cross-validated RMSE: 1.05\n",
      "        Feature  Importance\n",
      "3      WORF_PCT    0.161340\n",
      "4      TROI_PCT    0.154737\n",
      "0    PICARD_PCT    0.110649\n",
      "6    GEORDI_PCT    0.109638\n",
      "1     RIKER_PCT    0.083336\n",
      "10   GUINAN_PCT    0.078770\n",
      "9    WESLEY_PCT    0.072254\n",
      "11    TASHA_PCT    0.069263\n",
      "5   BEVERLY_PCT    0.068997\n",
      "2      DATA_PCT    0.039436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_default = simple_model_metrics(X_train, y_train, X_test, y_test, GradientBoostingRegressor(), False)\n",
    "gradient_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23605474-6184-41f9-a031-7291b7ef5073",
   "metadata": {},
   "source": [
    "Even though the training set is super close to a good fit, but the test and cross-validation scores questionable. This is a boosted model, and it requires more hyperparameter tuning to see positive results. \n",
    "\n",
    "--- \n",
    "\n",
    "### Summary of Baseline Simple Models\n",
    "Now we have tested our baseline regression models, let's take a quick look at the summary of all these default tests and determine which ones performed the best. Remember, our baseline RMSE is 0.94. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60c38b9f-dd87-4d3a-b327-58b782145fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Linear Regression</th>\n",
       "      <th>Decision Tree Regressor</th>\n",
       "      <th>Random Forest Regressor</th>\n",
       "      <th>SVM Regressor</th>\n",
       "      <th>OrdinalRidge (MORD)</th>\n",
       "      <th>LAD (MORD)</th>\n",
       "      <th>GradientBoost Regressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training RMSE</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Testing RMSE</th>\n",
       "      <td>0.87</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean Cross Validation Score</th>\n",
       "      <td>0.97</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Linear Regression Decision Tree Regressor  \\\n",
       "Training RMSE                            0.81                    0.00   \n",
       "Testing RMSE                             0.87                    1.14   \n",
       "Mean Cross Validation Score              0.97                    1.29   \n",
       "\n",
       "                            Random Forest Regressor SVM Regressor  \\\n",
       "Training RMSE                                  0.35          0.62   \n",
       "Testing RMSE                                   0.87          0.94   \n",
       "Mean Cross Validation Score                    0.93          0.87   \n",
       "\n",
       "                            OrdinalRidge (MORD) LAD (MORD)  \\\n",
       "Training RMSE                              0.85       0.88   \n",
       "Testing RMSE                               0.93       0.94   \n",
       "Mean Cross Validation Score                1.02       1.02   \n",
       "\n",
       "                            GradientBoost Regressor  \n",
       "Training RMSE                                  0.30  \n",
       "Testing RMSE                                   0.91  \n",
       "Mean Cross Validation Score                    1.05  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary of default/baseline models\n",
    "summary = pd.DataFrame(data = [training_score,testing_RMSE,mean_cross_val_score], index = ['Training RMSE', 'Testing RMSE', 'Mean Cross Validation Score'])\n",
    "summary.columns = ['Linear Regression', \n",
    "                   'Decision Tree Regressor', \n",
    "                   'Random Forest Regressor',\n",
    "                   'SVM Regressor',\n",
    "                   'OrdinalRidge (MORD)',\n",
    "                   'LAD (MORD)',\n",
    "                   'GradientBoost Regressor']\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc3c578-96cb-4ef5-9427-8a42d47dcb83",
   "metadata": {},
   "source": [
    "We're paying close attention to the cross validation scores, and comparing them to our baseline of 0.94. Ideally, we want this number to be as close to 0 as possible, as this means that it's nearing a perfect fit. The ones that shows the most promise are the Random Forest and SVM Regression models, and the ones that severely flopped were the Decision Tree Regressor (this is expected, as they have a tendency to overfit), and the GradientBoost Regressor (which at this stage is too early to tell, as it needs to be optimized further to show promise). \n",
    "\n",
    "## Quest to Find the Best Model\n",
    "\n",
    "To speed up our workflow, similar to our `simple_model_metrics()` function, another one was created for the sole purpose of regularisation and optimization. It'll take in the train-test data, the model type that we're tuning, and it's parameters predefined as a dictionary. It returns the best parameters, cross validation score on the test data, the testing RMSE and the feature importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "226c844b-b4d4-46e5-b189-1ce6d4e513ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return regularised model metrics\n",
    "def tuned_model_metrics(X_train, y_train, X_test, y_test, model, grid_params, parametric=True):\n",
    "    '''Takes in train test splits, model type, and grid parameters to return the  best model parameters, cross validation on test data score,\n",
    "    testing RMSE, as well as the feature importances'''\n",
    "\n",
    "    # grid search \n",
    "    gridsearch = GridSearchCV(model, # model type\n",
    "                              grid_params, # dictionary of parameters\n",
    "                              n_jobs=-1, # use all processors\n",
    "                              cv=5, # 5-fold cross validation\n",
    "                              verbose=2, # returns computation time for each fold \n",
    "                              error_score='raise') # value to assign if error occurs\n",
    "    \n",
    "    # fit the gridsearch on the X and y training sets\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "    \n",
    "    # print the best parameters from the gridsearch\n",
    "    print('Best parameters:', gridsearch.best_params_)\n",
    "    \n",
    "    \n",
    "    # print cross val score for test data \n",
    "    print('Cross-validated score:', '{0:0.2f}'.format(abs(gridsearch.best_score_)))\n",
    "    \n",
    "    # best model from the grid search (best_estimator_)\n",
    "    best_model = gridsearch.best_estimator_\n",
    "    \n",
    "    # print the testing RMSE from the best model \n",
    "    print('Testing RMSE:', '{0:0.2f}'.format(np.sqrt(metrics.mean_squared_error(y_test, best_model.predict(X_test)))))\n",
    "    \n",
    "    # conditions if parametric is true or false\n",
    "    if parametric == True:\n",
    "        \n",
    "        # prints dataframe containing the X train cols, best model coefficients, and absolute value of best model coefficients\n",
    "        print(pd.DataFrame(list(zip(X_train.columns, best_model.coef_, abs(best_model.coef_))), \n",
    "                 columns=['Feature', 'Coef', 'Abs Coef']).sort_values('Abs Coef', ascending=False).head(10))\n",
    "    else:\n",
    "        \n",
    "        # prints dataframe containing the X train cols and the best model's feature importances\n",
    "        print(pd.DataFrame(list(zip(X_train.columns, best_model.feature_importances_)), \n",
    "                 columns=['Feature', 'Importance']).sort_values('Importance', ascending=False).head(10))\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "94ab82b9-83b0-47ec-97c6-2ac4d3265391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bootstrap',\n",
       " 'ccp_alpha',\n",
       " 'criterion',\n",
       " 'max_depth',\n",
       " 'max_features',\n",
       " 'max_leaf_nodes',\n",
       " 'max_samples',\n",
       " 'min_impurity_decrease',\n",
       " 'min_impurity_split',\n",
       " 'min_samples_leaf',\n",
       " 'min_samples_split',\n",
       " 'min_weight_fraction_leaf',\n",
       " 'n_estimators',\n",
       " 'n_jobs',\n",
       " 'oob_score',\n",
       " 'random_state',\n",
       " 'verbose',\n",
       " 'warm_start']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of parameters to choose from\n",
    "rfr = ensemble.RandomForestRegressor()\n",
    "list(rfr.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f4594c-a668-48ee-bb66-ce9035b9e402",
   "metadata": {},
   "source": [
    "### Random Search Training\n",
    "\n",
    "In order to have an idea of which hyperparameters to tune, we need to have a clearer understanding of the pieces that go together in order to optimize the models. \n",
    "\n",
    "- n_estimators = number of trees in the foreset\n",
    "- max_features = max number of features considered for splitting a node\n",
    "- max_depth = max number of levels in each decision tree\n",
    "- min_samples_split = minimum number of data points placed in a node before the node is split\n",
    "- min_samples_leaf = minimum number of data points allowed in a leaf node\n",
    "- bootstrap = method for sampling data points (with or without replacement)\n",
    "\n",
    "We'll be using the `RandomizedSearchCV()` to help us define our hyperparameter grid that's best to operate in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "39d9bd58-04ca-434a-b1e9-96caa66974e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [1, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [1, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "092cffb7-d23d-4dab-b4f0-57511a59dabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [1, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 5 folds for cross val\n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e83906c5-c264-4b19-8fa7-cfe3beef0f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 400,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 90,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the best parameters \n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47eb438-f8a1-45e7-b1bf-9ea9c6fd0382",
   "metadata": {},
   "source": [
    "With the results from the Random Search,we'll impute similar ranges and see which values it outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a74391a-6ab5-4369-a67a-9f838f567859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4900 candidates, totalling 24500 fits\n",
      "Best parameters: {'bootstrap': True, 'max_depth': 27.5, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 0.505, 'n_estimators': 50}\n",
      "Cross-validated score on test data: 0.00\n",
      "Testing RMSE: 0.91\n",
      "        Feature  Importance\n",
      "4      TROI_PCT    0.232404\n",
      "3      WORF_PCT    0.148316\n",
      "9    WESLEY_PCT    0.111534\n",
      "10   GUINAN_PCT    0.101867\n",
      "11    TASHA_PCT    0.095879\n",
      "0    PICARD_PCT    0.074310\n",
      "5   BEVERLY_PCT    0.061559\n",
      "1     RIKER_PCT    0.061447\n",
      "12  PULASKI_PCT    0.042971\n",
      "2      DATA_PCT    0.038850\n"
     ]
    }
   ],
   "source": [
    "rfr_parameters = {'bootstrap': [True, False],\n",
    "             'max_depth': np.linspace(5, 50, 5),\n",
    "             'max_features': ['auto', 'sqrt'],\n",
    "             'min_samples_leaf': range(1,2,4),\n",
    "             'min_samples_split': np.linspace(0.01, 1, 5),\n",
    "             'n_estimators': range(10, 500, 10)}\n",
    "\n",
    "rfr_tuning_fork1 = tuned_model_metrics(X_train, y_train, X_test, y_test, rfr, rfr_parameters, parametric=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134745f6-1dfb-43d6-8d54-8ab51dcb2906",
   "metadata": {},
   "source": [
    "We can see that the result of the grid search with it's various parameter options that with the best params, the results of cross val of 0 and the testing RMSE of 0.91... these parameters don't perform so well. \n",
    "\n",
    "Moving forward, we'll follow the advice of the Random Search and impute values that it suggested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f757ead5-715d-497c-a0ed-7793a761271b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
      "Best parameters: {'bootstrap': True, 'max_depth': 100, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 12, 'n_estimators': 100}\n",
      "Cross-validated score: 0.03\n",
      "Testing RMSE: 0.88\n",
      "        Feature  Importance\n",
      "3      WORF_PCT    0.160194\n",
      "4      TROI_PCT    0.147402\n",
      "0    PICARD_PCT    0.120266\n",
      "1     RIKER_PCT    0.107395\n",
      "2      DATA_PCT    0.083824\n",
      "5   BEVERLY_PCT    0.082592\n",
      "6    GEORDI_PCT    0.077717\n",
      "9    WESLEY_PCT    0.077212\n",
      "11    TASHA_PCT    0.068782\n",
      "10   GUINAN_PCT    0.049226\n"
     ]
    }
   ],
   "source": [
    "rfr_parameters = {'bootstrap': [True],\n",
    "                  'max_depth': [80, 90, 100, 110],\n",
    "                  'max_features': ['sqrt'],\n",
    "                  'min_samples_leaf': [3, 4, 5],\n",
    "                  'min_samples_split': [8, 10, 12],\n",
    "                  'n_estimators': [100, 200, 300, 1000]}\n",
    "\n",
    "rfr_tuning_fork2 = tuned_model_metrics(X_train, y_train, X_test, y_test, rfr, rfr_parameters, parametric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a252de4e-19d2-4bd9-bf3a-9c14cc60ecb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Best parameters: {'bootstrap': True, 'max_depth': 110, 'max_features': 'sqrt', 'min_samples_leaf': 6, 'min_samples_split': 12, 'n_estimators': 100}\n",
      "Cross-validated score: 0.02\n",
      "Testing RMSE: 0.86\n",
      "        Feature  Importance\n",
      "4      TROI_PCT    0.151029\n",
      "0    PICARD_PCT    0.134636\n",
      "3      WORF_PCT    0.121503\n",
      "11    TASHA_PCT    0.101477\n",
      "6    GEORDI_PCT    0.090237\n",
      "5   BEVERLY_PCT    0.090206\n",
      "2      DATA_PCT    0.089824\n",
      "1     RIKER_PCT    0.086843\n",
      "9    WESLEY_PCT    0.081470\n",
      "10   GUINAN_PCT    0.041491\n"
     ]
    }
   ],
   "source": [
    "rfr_parameters = {'bootstrap': [True],\n",
    "                  'max_depth': [100, 110, None],\n",
    "                  'max_features': ['sqrt'],\n",
    "                  'min_samples_leaf': [5,6,7],\n",
    "                  'min_samples_split': [10, 12, 14],\n",
    "                  'n_estimators': [100, 125, 150]}\n",
    "\n",
    "rfr_tuning_fork3 = tuned_model_metrics(X_train, y_train, X_test, y_test, rfr, rfr_parameters, parametric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "33ba7e63-47db-4aa8-bc25-9e092be8b068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Best parameters: {'bootstrap': True, 'max_depth': 100, 'max_features': 'sqrt', 'min_samples_leaf': 7, 'min_samples_split': 15, 'n_estimators': 125}\n",
      "Cross-validated score: 0.02\n",
      "Testing RMSE: 0.87\n",
      "        Feature  Importance\n",
      "4      TROI_PCT    0.185998\n",
      "3      WORF_PCT    0.154935\n",
      "0    PICARD_PCT    0.105093\n",
      "11    TASHA_PCT    0.099390\n",
      "2      DATA_PCT    0.089928\n",
      "5   BEVERLY_PCT    0.087929\n",
      "1     RIKER_PCT    0.085922\n",
      "6    GEORDI_PCT    0.073447\n",
      "9    WESLEY_PCT    0.064797\n",
      "10   GUINAN_PCT    0.039133\n"
     ]
    }
   ],
   "source": [
    "rfr_parameters = {'bootstrap': [True],\n",
    "                  'max_depth': [100, 150, 200],\n",
    "                  'max_features': ['sqrt'],\n",
    "                  'min_samples_leaf': [5,7,9],\n",
    "                  'min_samples_split': [10, 15, 20],\n",
    "                  'n_estimators': [125, 150, 175]}\n",
    "\n",
    "rfr_tuning_fork4 = tuned_model_metrics(X_train, y_train, X_test, y_test, rfr, rfr_parameters, parametric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b5210fa5-44a2-4a5c-8c73-137c2b7b702d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Best parameters: {'bootstrap': True, 'max_depth': 300, 'max_features': 'sqrt', 'min_samples_leaf': 10, 'min_samples_split': 30, 'n_estimators': 225}\n",
      "Cross-validated score: 0.02\n",
      "Testing RMSE: 0.90\n",
      "        Feature  Importance\n",
      "4      TROI_PCT    0.193761\n",
      "3      WORF_PCT    0.171820\n",
      "5   BEVERLY_PCT    0.105818\n",
      "1     RIKER_PCT    0.099816\n",
      "0    PICARD_PCT    0.097513\n",
      "9    WESLEY_PCT    0.084470\n",
      "2      DATA_PCT    0.077259\n",
      "6    GEORDI_PCT    0.074613\n",
      "11    TASHA_PCT    0.063889\n",
      "10   GUINAN_PCT    0.028675\n"
     ]
    }
   ],
   "source": [
    "rfr_parameters = {'bootstrap': [True],\n",
    "                  'max_depth': [100, 200, 300],\n",
    "                  'max_features': ['sqrt'],\n",
    "                  'min_samples_leaf': [5, 10, 15],\n",
    "                  'min_samples_split': [10, 20, 30],\n",
    "                  'n_estimators': [125, 225, 325]}\n",
    "\n",
    "rfr_tuning_fork5 = tuned_model_metrics(X_train, y_train, X_test, y_test, rfr, rfr_parameters, parametric=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccadb92-0308-4f0c-9b6a-5a10b74418bc",
   "metadata": {},
   "source": [
    "Let's do one last gridsearch, where the n_estimators are significantly increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc46cb2d-05df-402e-a87d-8f20a481d1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2736 candidates, totalling 13680 fits\n"
     ]
    }
   ],
   "source": [
    "rfr_parameters = {'bootstrap': [True, False],\n",
    "             'max_depth': range(100,500, 50),\n",
    "             'max_features': ['sqrt'],\n",
    "             'min_samples_leaf': [5,10,15],\n",
    "             'min_samples_split': [10, 15, 20],\n",
    "             'n_estimators': range(50, 1000, 50)}\n",
    "\n",
    "rfr_tuning_fork5 = tuned_model_metrics(X_train, y_train, X_test, y_test, rfr, rfr_parameters, parametric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01079a07-bed5-460b-beec-3ccd00114915",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'bootstrap': [True, False],\n",
    "             'max_depth': np.linspace(5, 50, 5),\n",
    "             'max_features': ['auto', 'sqrt'],\n",
    "             'min_samples_leaf': range(1,2,4),\n",
    "             'min_samples_split': np.linspace(0.01, 1, 5),\n",
    "             'n_estimators': range(10, 500, 10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da899805-9b76-448e-84c7-7a6eb7029869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba80e8a-3b3b-4ebe-883e-6eb24e2eb602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
